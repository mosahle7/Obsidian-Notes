
**HHH:** Helpful, Honest, Harmless

**Reinforcement Learning from Human Feedback (RLHF):**

Uses reinforcement learning to finetune the LLM with human feedback data, resulting in a model that is better aligned with human preferences. 

RLHF to make sure that your model produces outputs that 
maximize usefulness and relevance to the input prompt. 

RLHF can help minimize the potential for harm. You can train your model to give caveats that acknowledge their limitations and to avoid toxic language and topics.