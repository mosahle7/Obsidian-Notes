
**HHH:** Helpful, Honest, Harmless

**Reinforcement Learning from Human Feedback (RLHF):**

Uses reinforcement learning to finetune the LLM with human feedback data, resulting in a model that is better aligned with human preferences. 

RLHF to make sure that your model produces outputs that maximize usefulness and relevance to the input prompt. 

RLHF can help minimize the potential for harm. You can train your model to give caveats that acknowledge their limitations and to avoid toxic language and topics.

The agent's policy that guides the actions is the LLM, and its objective is to generate text that is perceived as being aligned with the human preferences. 

The environment is the context window of the model, the space in which text can be entered via a prompt. The state that the model considers before taking an action is the current context.

The action here is the act of generating text. The action space is the token vocabulary.

The action that the model will take, meaning which token it will choose next, depends on the prompt text in the context and the probability distribution over the vocabulary space. The reward is assigned based on how closely the completions align with human preferences.

**RL Algorithm:** **Proximal Policy Optimization  (PPO)**, **Q-Learning**

**Potential Problem - Reward Hacking:**

The agent learns to cheat the system by favoring actions that maximize the reward received even if those actions don't align well with the original objective. 

To prevent reward hacking from happening, use the initial instruct LLM as performance reference (reference model). The weights of the reference model are frozen and are not updated during iterations of RHF. This way, it maintains a single reference model to compare to. During training, each prompt is passed to both models, generating a completion by the reference LLM and the intermediate LLM updated model.

Compare the two completions and calculate a value called the **Kullback-Leibler divergence, or KL divergence**, a statistical measure of how different two probability distributions are. It is calculated for each generated token across the whole vocabulary of the LLM. Using a softmax function, the number of probabilities are reduced to much less than the full vocabulary size. 

After KL divergence between the two models are calculated, a penalty term is added to the reward calculation. This will penalize the RL updated model if it shifts too far from the reference LLM and generates completions that are too different.

**Scaling Human Feedback:**

**Model self-supervision - Constitutional AI:**

A method for training models using a set of rules and principles that govern the model's behavior.

Together with a set of sample prompts, these form the constitution. Constitutional AI is useful not only for scaling feedback, it can also help address some unintended consequences of RLHF.

When implementing the Constitutional AI method, the model is trained in two distinct phases. In the first stage, **supervised learning** is carried out, prompting the model in ways designed to elicit harmful responses, this process is called **red teaming**. The model is then asked to critique its own harmful responses according to constitutional principles and revise them to comply with those rules. Finally, the model is fine-tuned using pairs of red team prompts and the revised constitutional responses.

The second part of the process performs **reinforcement learning**. This stage is similar to RLHF, except that instead of human feedback, feedback is generated by a model. This is referred to as **reinforcement learning from AI feedback (RLAIF)**. The fine-tuned model from the previous step is used to generate a set of responses to a prompt. The model is then asked which of the responses is preferred according to the constitutional principles. The result is a model-generated preference dataset that can be used to train a reward model. With this reward model, the model can be further fine-tuned using a reinforcement learning algorithm such as PPO.

**LLM Optimization Techniques:**

**Distillation:**

A technique that focuses on having a larger teacher model train a smaller student model.

 Student model learns to statistically mimic the behavior of the teacher model, just in the final prediction layer or in the model's hidden layers as well.

 Freezing teacher model weights, it is used to generate completions for training data. At same time, completions are generated for training data using student model. 
 
 Knowledge distillation between teacher and student model is achieved by minimizing a loss function called **distillation loss**. To calculate this loss, distillation uses probability distribution over tokens produced by teacher model softmax layer. Since teacher model is already fine-tuned on training data, probability distribution likely closely matches ground truth data and does not have much variation in tokens. To address this, distillation applies a trick by adding temperature parameter to softmax function. With temperature parameter greater than one, probability distribution becomes broader and less strongly peaked. This softer distribution provides a set of tokens similar to ground truth tokens.

- **Teacher model output** → **Soft Labels** (probability distribution via softmax with temperature).
- **Student model prediction** → **Soft Predictions** (probability distribution via softmax with temperature).

In parallel, the student model is trained to generate correct predictions based on ground truth training data. Here, temperature setting is not varied, and the standard softmax function is used.

- **Ground truth data (true targets)** → **Hard Labels** (one-hot encoded or discrete tokens).
- **Student model argmax output** → **Hard Predictions** (most probable token per position).

Loss between two is **student loss**. Combined distillation and student losses are used to update weights of student model via back propagation. 

Not effective for generative decoder models, more effective for encoder only models, such as BERT, that have a lot of representation redundancy.

Not reducing model size.
 
**Post-Training Quantization (PTQ):**

Transforms model weights to a lower precision representation (16 bit FP/ 8 bit FP)

Can be applied to weights or both weights and activation layers.

Requires extra calibration step to statistically capture dynamic range of original parameter values.

Sometimes small percentage reduction in model evaluation metrics.

**Pruning:**

Reduce model size for inference by eliminating weights that are not contributing much to overall model performance (0 or very close to 0).

Some requires full retraining of model.
Others fall into category of PEFT like LoRA.

Reduces size of model and improves performance.

**Difficulties:**

- **Out of Date**
- **Mathematical Calculations**
- **Hallucination:** Tendency to generate text even when they don't know answer to a problem.

**Retrieval Augmented Generation (RAG):**

Framework  for building LLM powered systems that make use of external data sources and applications to overcome some of the limitations of models.

Providing your model with external information, can improve both relevance and accuracy of its completions.

Model component called **Retriever**, which consists of a **query encoder and an external data source**.

The encoder takes the user's input prompt and encodes it into a form that can be used to query the data source.

These two components are trained together to find documents within external data that are most relevant to the input query.  Retriever returns the best single or group of documents from the data source and combines the new information with the original user query. The new expanded prompt is then passed to the language model, which 
generates a completion that makes use of the data.

RAG architectures can be used to integrate multiple types of external information sources like local documents, wikis, expert systems, webpages, databases, **Vector Store**

**Vector Store:** Contains vector representations of text. Useful data format for language models. Enables fast and efficient kind of relevant search based on similarity

**Vector Databases**: Particular implementation of Vector Store, where each vector is identified by a key. Enables a citation to be included in completion.

**Requirements for using LLMs to Power Applications:**

- Plan Actions
- Format Outputs
- Validate Actions

**Chain of Thought Prompting**

**Program-Aided Language Model (PAL):**

Pairs an LLM with an external code interpreter to carry out calculations. The method makes use of chain of thought prompting to generate executable Python scripts. Then, they are passed to an interpreter to execute.

For overcoming mistakes in solving math problems.

**ReAct (Reasoning and Action):**

A prompting strategy that combines chain of thought reasoning with action planning.

ReAct uses structured examples to show a large language model how to reason through a problem and decide on actions to take that move it closer to a solution.

- **Question:** Example prompts start with a question that will require multiple steps to answer.

- **Thought:**  A reasoning step that demonstrates to the model how to  tackle the problem and identify an **Action** to take.

- **Observation:** This is where new information provided by the external search is brought into the context of the prompt.

- For the model to interpret the prompt then repeats the cycle as many 
- times as is necessary to obtain the final answer. 

**LangChain**:

Framework that provides with modular pieces that contain components necessary to work with LLMs. These components include **prompt templates** for many different use cases that can be used to format both input examples and model completions and **memory** to store interactions with an LLM. It also includes **pre-built tools** that enable you to carry out  a wide variety of tasks, including calls to external datasets and various APIs. 

Connecting a selection of these individual components together results in a chain.

**Agent:** Used to interpret input from user and determine which tools to use to complete the task.

**Model Hub: AWS Sagemaker JumpStart**

