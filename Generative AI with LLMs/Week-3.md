
**HHH:** Helpful, Honest, Harmless

**Reinforcement Learning from Human Feedback (RLHF):**

Uses reinforcement learning to finetune the LLM with human feedback data, resulting in a model that is better aligned with human preferences. 

RLHF to make sure that your model produces outputs that 
maximize usefulness and relevance to the input prompt. 

RLHF can help minimize the potential for harm. You can train your model to give caveats that acknowledge their limitations and to avoid toxic language and topics.

The agent's policy that guides the actions is the LLM, and its objective is to generate text that is perceived as being aligned with the human preferences. 

The environment is the context window of the model, the space in which text can be entered via a prompt. The state that the model considers before taking an action is the current context.

The action here is the act of generating text. The action space is the token vocabulary.

The action that the model will take, meaning which token it will choose next, depends on the prompt text in the context and the probability distribution over the vocabulary space. The reward is assigned based on how closely the completions align with human preferences.

**RL Algorithm:** **Proximal Policy Optimization  (PPO)**, **Q-Learning**

**Potential Problem - Reward Hacking:**

The agent learns to cheat the system by favoring actions that maximize the reward received even if those actions don't align well with the original objective. 

To prevent reward hacking from happening, use the initial instruct LLM as performance reference (reference model). The weights of the reference model are frozen and are not updated during iterations of RHF. This way, it maintains a single reference model to compare to. During training, each prompt is passed to both models, generating a completion by the reference LLM and the intermediate LLM updated model.

Compare the two completions and calculate a value called the **Kullback-Leibler divergence, or KL divergence**, a statistical measure of how different two probability distributions are. It is calculated for each generated token across the whole vocabulary of the LLM. Using a softmax function, the number of probabilities are reduced to much less than the full vocabulary size. 

After KL divergence between the two models are calculated, a penalty term is added to the reward calculation. This will penalize the RL updated model if it shifts too far from the reference LLM and generates completions that are too different.

**Scaling Human Feedback:**

**Model self-supervision - Constitutional AI:**

A method for training models using a set of rules and principles that govern the model's behavior.

Together with a set of sample prompts, these form the constitution. Constitutional AI is useful not only for scaling feedback, it can also help address some unintended consequences of RLHF.

When implementing the Constitutional AI method, the model is trained in two distinct phases. In the first stage, **supervised learning** is carried out, prompting the model in ways designed to elicit harmful responses, this process is called **red teaming**. The model is then asked to critique its own harmful responses according to constitutional principles and revise them to comply with those rules. Finally, the model is fine-tuned using pairs of red team prompts and the revised constitutional responses.

The second part of the process performs **reinforcement learning**. This stage is similar to RLHF, except that instead of human feedback, feedback is generated by a model. This is referred to as **reinforcement learning from AI feedback (RLAIF)**. The fine-tuned model from the previous step is used to generate a set of responses to a prompt. The model is then asked which of the responses is preferred according to the constitutional principles. The result is a model-generated preference dataset that can be used to train a reward model. With this reward model, the model can be further fine-tuned using a reinforcement learning algorithm such as PPO.

**LLM Optimization Techniques:**

**Distillation:**

A technique that focuses on having a larger teacher model train a smaller student model.

 Student model learns to statistically mimic the behavior of the teacher model, just in the final prediction layer or in the model's hidden layers as well.

 Freezing teacher model weights, it is used to generate completions for training data. At same time, completions are generated for training data using student model. 
 
 Knowledge distillation between teacher and student model is achieved by minimizing a loss function called **distillation loss**. To calculate this loss, distillation uses probability distribution over tokens produced by teacher model softmax layer. Since teacher model is already fine-tuned on training data, probability distribution likely closely matches ground truth data and does not have much variation in tokens. To address this, distillation applies a trick by adding temperature parameter to softmax function. With temperature parameter greater than one, probability distribution becomes broader and less strongly peaked. This softer distribution provides a set of tokens similar to ground truth tokens.

- **Teacher model output** → **Soft Labels** (probability distribution via softmax with temperature).
- **Student model prediction** → **Soft Predictions** (probability distribution via softmax with temperature).

In parallel, the student model is trained to generate correct predictions based on ground truth training data. Here, temperature setting is not varied, and the standard softmax function is used.

- **Ground truth data (true targets)** → **Hard Labels** (one-hot encoded or discrete tokens).
- **Student model argmax output** → **Hard Predictions** (most probable token per position).

Loss between two is **student loss**. Combined distillation and student losses are used to update weights of student model via back propagation. 

Not effective for generative decoder models, more effective for encoder only models, such as BERT, that have a lot of representation redundancy.

Not reducing model size.

**Post-Training Quantization (PTQ):**

